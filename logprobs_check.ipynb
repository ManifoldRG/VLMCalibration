{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: This is to check what each model outputs as logprobs when given a question - and if the behaviour is different for:\n",
    "1. Quants vs float16 variants\n",
    "2. Between model families (Llama vs Qwen vs Gemma)\n",
    "3. Between model sizes (14B vs 8B vs 3B vs 1B)\n",
    "'''\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "TRUE_SYNONYMS = [\n",
    "    \"true\",\n",
    "    \"correct\",\n",
    "    \"truth\",\n",
    "    \"yes\",\n",
    "    \"right\",\n",
    "    \"verdade\",\n",
    "]\n",
    "\n",
    "FALSE_SYNONYMS = [\n",
    "    \"false\",\n",
    "    \"incorrect\",\n",
    "    \"wrong\",\n",
    "    \"fake\",\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"none\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_request(payload, url=None, max_retries=3, retry_delay=1):\n",
    "    if url is None:\n",
    "        raise ValueError(\"URL is not set\")\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "            response.raise_for_status()\n",
    "            if attempt > 0:  # Only print success if we had to retry\n",
    "                print(\"\\033[92mRequest succeeded after retry\\033[0m\")  # Green text\n",
    "            return response.json()\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                print(f\"\\033[91mConnection error on attempt {attempt}/{max_retries}. Retrying in {retry_delay}s...\\033[0m\")  # Red text\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"\\033[91mFinal connection error after {max_retries} attempts: {e}\\033[0m\")  # Red text\n",
    "                print(\"Connection error occurred. Continuing with available results.\")\n",
    "                exit(1)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                print(f\"\\033[91mRequest error on attempt {attempt}/{max_retries}. Retrying in {retry_delay}s...\\033[0m\")  # Red text\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"\\033[91mFinal request error after {max_retries} attempts: {e}\\033[0m\")  # Red text\n",
    "                print(payload)\n",
    "                return None\n",
    "\n",
    "\n",
    "def get_initial_response(task):\n",
    "    try:\n",
    "        url = task[\"url\"]\n",
    "        temp = task[\"temp\"]\n",
    "        max_tokens = task[\"max_tokens\"]\n",
    "        question_str = task[\"question_str\"]\n",
    "\n",
    "        payload = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question_str},\n",
    "            ],\n",
    "            \"temperature\": temp,\n",
    "            \"max_tokens\": max_tokens,\n",
    "        }\n",
    "\n",
    "        response = send_request(payload, url)\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        task[\"response_text\"] = response_text\n",
    "        return task\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting initial response for task: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def get_confidence_score(task):\n",
    "    \"\"\"Ask the model to self-evaluate and extract token-level log-probs.\"\"\"\n",
    "    try:\n",
    "        url = task[\"url\"]\n",
    "        temp = task[\"temp\"]\n",
    "        question_str = task[\"question_str\"]\n",
    "        response_text = task[\"response_text\"]\n",
    "        exp_type = task[\"exp_type\"]\n",
    "        model_name = task[\"model_name\"]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question_str},\n",
    "            {\"role\": \"assistant\", \"content\": response_text},\n",
    "        ]\n",
    "\n",
    "        if exp_type == \"cot_exp\":\n",
    "            cot_text = task[\"cot_text\"]\n",
    "            messages.extend([\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"Before answering whether your above answer is correct, \"\n",
    "                        \"please provide a detailed chain-of-thought explanation of \"\n",
    "                        \"your reasoning. Explain step-by-step how you arrived at \"\n",
    "                        \"your answer and why you think it is correct or might be \"\n",
    "                        \"incorrect.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": cot_text},\n",
    "            ])\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Is the above answer correct? Answer only with the single word 'true' or 'false'.\",\n",
    "        })\n",
    "\n",
    "        # Format request according to vLLM OpenAI-compatible API\n",
    "        payload = {\n",
    "            # \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"max_tokens\": 1,\n",
    "            \"logprobs\": True,\n",
    "            \"top_logprobs\": 20\n",
    "        }\n",
    "\n",
    "        response = send_request(payload, url)\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        # Extract logprobs from the response\n",
    "        if \"choices\" in response and len(response[\"choices\"]) > 0:\n",
    "            choice = response[\"choices\"][0]\n",
    "            if \"logprobs\" in choice and \"content\" in choice[\"logprobs\"]:\n",
    "                # Get the logprobs for the tokens\n",
    "                logprobs = []\n",
    "                for token_info in choice[\"logprobs\"][\"content\"]:\n",
    "                    if \"top_logprobs\" in token_info:\n",
    "                        for top_prob in token_info[\"top_logprobs\"]:\n",
    "                            token = top_prob[\"token\"].lower()\n",
    "                            if any(syn in token for syn in TRUE_SYNONYMS):\n",
    "                                print(f\"\\033[92m{token}\\033[0m (true)\")\n",
    "                                logprobs.append({\n",
    "                                    \"token\": \"true\",\n",
    "                                    \"logprob\": top_prob[\"logprob\"]\n",
    "                                })\n",
    "                            elif any(syn in token for syn in FALSE_SYNONYMS):\n",
    "                                print(f\"\\033[94m{token}\\033[0m (false)\")\n",
    "                                logprobs.append({\n",
    "                                    \"token\": \"false\", \n",
    "                                    \"logprob\": top_prob[\"logprob\"]\n",
    "                                })\n",
    "                            else:\n",
    "                                print(f\"\\033[91mShit that did not fall in the true or false category: {token}\\033[0m\")\n",
    "                \n",
    "                task[\"logprobs\"] = logprobs\n",
    "                return task\n",
    "\n",
    "        print(f\"Unexpected response structure: {response}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting confidence score: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e69a9f28be545519a175f5ad421bebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88121869d6ad4fbba751374a47067b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d167ee1e0a74091ae1e53c64f8c2009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10af23eff02466d96d2ae361485787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3021181ba9bb4804b6e30381c8cc35f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 'test', 'dataset_type': 'gsm8k', 'dataset_split': 'test', 'exp_type': 'zs_exp', 'url': 'http://localhost:8000/v1/chat/completions', 'temp': 0.0, 'max_tokens': 1024, 'model_name': 'llama 3.2 3b', 'question_str': 'Given the following problem, reason and give a final answer to the problem.\\nProblem: Oscar has 24 lollipops and eats 2 on his way to school.  He passes 14 out to his friends.  He buys twice as many lollipops on his way home as he gave to his friends.  He eats 3 more that night and 2 more in the morning.  How many lollipops does Oscar have?\\nYour response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.', 'response_text': \"To solve this problem, let's break it down step by step.\\n\\n1. Oscar starts with 24 lollipops and eats 2 on his way to school. \\n   24 - 2 = 22 lollipops\\n\\n2. He passes 14 out to his friends. \\n   22 - 14 = 8 lollipops\\n\\n3. He buys twice as many lollipops on his way home as he gave to his friends. \\n   Since he gave 14 lollipops to his friends, he buys 2 * 14 = 28 lollipops.\\n   8 + 28 = 36 lollipops\\n\\n4. He eats 3 more that night and 2 more in the morning. \\n   36 - 3 - 2 = 31 lollipops\\n\\nThe final answer is 31.\"}\n"
     ]
    }
   ],
   "source": [
    "from utils import format_gsm8k_question\n",
    "\n",
    "gsm8k_question = format_gsm8k_question(dataset['test']['question'][327])\n",
    "\n",
    "task = {\n",
    "    \"idx\": \"test\",\n",
    "    \"dataset_type\": \"gsm8k\",\n",
    "    # \"dataset\": dataset,\n",
    "    \"dataset_split\": \"test\",\n",
    "    \"exp_type\": \"zs_exp\",\n",
    "    \"url\": \"http://localhost:8000/v1/chat/completions\",\n",
    "    \"temp\": 0.0,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"model_name\": \"llama 3.2 3b\",\n",
    "    \"question_str\": gsm8k_question,\n",
    "}\n",
    "\n",
    "task = get_initial_response(task)\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following problem, reason and give a final answer to the problem.\n",
      "Problem: Oscar has 24 lollipops and eats 2 on his way to school.  He passes 14 out to his friends.  He buys twice as many lollipops on his way home as he gave to his friends.  He eats 3 more that night and 2 more in the morning.  How many lollipops does Oscar have?\n",
      "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
      "To solve this problem, let's break it down step by step.\n",
      "\n",
      "1. Oscar starts with 24 lollipops and eats 2 on his way to school. \n",
      "   24 - 2 = 22 lollipops\n",
      "\n",
      "2. He passes 14 out to his friends. \n",
      "   22 - 14 = 8 lollipops\n",
      "\n",
      "3. He buys twice as many lollipops on his way home as he gave to his friends. \n",
      "   Since he gave 14 lollipops to his friends, he buys 2 * 14 = 28 lollipops.\n",
      "   8 + 28 = 36 lollipops\n",
      "\n",
      "4. He eats 3 more that night and 2 more in the morning. \n",
      "   36 - 3 - 2 = 31 lollipops\n",
      "\n",
      "The final answer is 31.\n"
     ]
    }
   ],
   "source": [
    "print(task['question_str'])\n",
    "print(task[\"response_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idx', 'dataset_type', 'dataset_split', 'exp_type', 'url', 'temp', 'max_tokens', 'model_name', 'question_str', 'response_text'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task['exp_type'] = \"zs_exp\"\n",
    "task.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mfalse\u001b[0m (false)\n",
      "\u001b[92mtrue\u001b[0m (true)\n",
      "\u001b[94mfalse\u001b[0m (false)\n",
      "\u001b[92mtrue\u001b[0m (true)\n",
      "\u001b[94mfalse\u001b[0m (false)\n",
      "\u001b[92mġuntrue\u001b[0m (true)\n",
      "\u001b[92mtrue\u001b[0m (true)\n",
      "\u001b[94m_false\u001b[0m (false)\n",
      "\u001b[94m(false\u001b[0m (false)\n",
      "\u001b[94m/false\u001b[0m (false)\n",
      "\u001b[94m.false\u001b[0m (false)\n",
      "\u001b[94mġfalse\u001b[0m (false)\n",
      "\u001b[92m_true\u001b[0m (true)\n",
      "\u001b[92m(true\u001b[0m (true)\n",
      "\u001b[94m=false\u001b[0m (false)\n",
      "\u001b[92mincorrect\u001b[0m (true)\n",
      "\u001b[94mno\u001b[0m (false)\n",
      "\u001b[91mShit that did not fall in the true or false category: the\u001b[0m\n",
      "\u001b[94mwrong\u001b[0m (false)\n",
      "\u001b[94mĉfalse\u001b[0m (false)\n",
      "Probability of true: 0.07492146896772653\n"
     ]
    }
   ],
   "source": [
    "task = get_confidence_score(task)\n",
    "probs = {\"true\": 0.0, \"false\": 0.0}\n",
    "\n",
    "for item in task['logprobs']:\n",
    "    token = item[\"token\"].lower()\n",
    "    is_true_synonym = any(synonym in token for synonym in TRUE_SYNONYMS)\n",
    "    is_false_synonym = any(synonym in token for synonym in FALSE_SYNONYMS)\n",
    "\n",
    "    if is_true_synonym:\n",
    "        probs[\"true\"] += np.exp(item[\"logprob\"])\n",
    "    elif is_false_synonym:\n",
    "        probs[\"false\"] += np.exp(item[\"logprob\"])\n",
    "\n",
    "p_true = (\n",
    "    probs[\"true\"] / (probs[\"true\"] + probs[\"false\"])\n",
    "    if (probs[\"true\"] + probs[\"false\"]) > 0\n",
    "    else 0.0\n",
    ")\n",
    "print(f\"Probability of true: {p_true}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
